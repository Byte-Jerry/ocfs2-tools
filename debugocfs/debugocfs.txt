* introduction
  * what is ocfs?
  * on-disk structures and layout
  * using debugocfs to inspect on-disk structures
* explanation of features
  * -h : volume header
  * -g : global bitmap
  * -v : vote sector
  * -p : publish sector
  * -f : ocfs_file_entry structure
  * -F : ocfs_file_entry and ocfs_extent_group structures
  * -d : ocfs_dir_node structure
  * -a : file allocation system file
  * -A : dir allocation system file
  * -b : file allocation bitmap system file
  * -B : dir allocation bitmap system file
  * -r : recover log file system file
  * -c : cleanup log system file
  * -L : vol metadata log system file
  * -M : vol metadata system file
* usage scenarios
  * Before you get started
  * Erratic behavior or unable to mount
  * File size, last modified, etc. for a file is incorrect
  * Deleted file shows in a directory
  * Non-deleted file does not show
  * Single filename shows up more than once
  * Hang occurs while extending/truncating/removing a datafile
* Disclaimer



==============================================================

* introduction
  * what is ocfs?

The Oracle Clustered Filesystem (ocfs) is an open-source project created
by Oracle to allow RAC database administrators to more easily create and
set up clustered databases without having to configure raw partitions.
Unlike raw partitions, the user is presented with a normal filesystem
interface, and is able to use most normal filesystem structures such as
directories, regular files and symlinks (hard links are not currently
implemented).  Database administration activities that are often
complicated by the use of raw partitions are simplified by the use of
ocfs, such as resizing, truncating, adding or deleting datafiles.
Additionally, ocfs is an extent-based filesystem which allows for large
contiguous allocations, optimal for files such as oracle datafiles.

There are several differences between ocfs and a normal local filesystem
(such as ext3) that should be taken into consideration.  While a local
filesystem is able to keep most metadata and data cached in various
caches at any point in time, ocfs needs to communicate all metadata
changes to any other mounted nodes when they occur, and for oracle use,
will also synchronously commit all data to disk as well.  This limits
ocfs with respect to use as a general-purpose filesystem because
accesses to ordinary file metadata such as file size or last modified
date may need to do an IO in order to fetch the information.  This extra
IO will slow down such accesses considerably.

Another important consideration is that since ocfs is a clustered
filesystem, it must not only take local locks when making metadata
changes (using semaphores, etc), but it must also cooperatively lock
between nodes using a distributed lock manager.  Meanwhile, for crash
recovery, ocfs also needs to journal critical changes to disk so that
filesystem structures are not corrupted.  The lock manager (called a
DLM) which ocfs uses is a simple lock manager which currently uses only
the ocfs disk partition itself to communicate using a publish-vote
mechanism.  Each node intending to lock a piece of metadata on disk will
publish its intent to lock to a specific location on disk.  Meanwhile,
all nodes will continuously read other nodes' published information and
cast a vote regarding whether or not there is a conflict that should
prevent the requestor from getting the lock.

  * on-disk structures and layout

To ensure proper coordination between nodes, ocfs defines a rigid
on-disk format and specific on-disk structures.

+-+-----+-----+-----+-----------+-------+-------+----
| |     |     |     |           |       |       | ...
+-+-----+-----+-----+-----------+-------+-------+----
 ^   ^     ^     ^        ^         ^       ^     
 |   |     |     |        |         |       |
 |   |     |     |        |         |       +-- dir hierarchy
 |   |     |     |        |         +-- system files (see below)
 |   |     |     |        +-- global bitmap 
 |   |     |     +-- vote sectors (32)
 |   |     +-- publish sectors (32)
 |   +-- autoconfig sectors
 +-- disk header


The disk header sector acts as the superblock of an ocfs filesystem,
storing all of the vital information for properly bootstrapping the
mount.  The disk header stores pointers to each of the following disk
structures.  If this should be corrupted, you may be unable to mount the
filesystem or the filesystem may be corrupted.

The autoconfig sectors are used by ocfs to automatically assign
identities to new nodes as they attach to an ocfs volume.  There are a
total of 32 slots available, so each node will receive a nodenum between
0 and 31.  For a given mount you can look at /proc/ocfs/###/nodenum to
determine your node's number for that volume.  The ### in this case is a
sequential mount id (which only has meaning on a single node), and to
determine the mountpoint for this mount you can dump
/proc/ocfs/###/mountpoint.  The first node to mount an ocfs volume after
formatting will always be node zero, and no other nodes are allowed to
attach to this volume while the ocfs partition is prepared the first
time.

After the autoconfig sectors, there are 32 publish sectors followed by
32 vote sectors.  Upon mounting, a node mangager daemon (ex. [ocfsnm-0])
will be started and will continually update a timestamp in the publish
sector for this node, as well as read each of the other publish sectors.
This allows for one daemon to handle both node monitoring (checking if a
node is dead and needs recovery) and lock management simultaneously, at
approximately 1/2 second intervals.  If the ocfsnm detects that a node
has requested a vote for a certain resource, it will find the status of
that resource on the local node to determine whether or not to allow the
other node to take the lock.  Once the determination has been made, the
ocfsnm will write its response into the corresponding vote sector, and
the process which requested the lock on the other node will collect up
all responses and decide whether the lock can be taken, cannot be taken,
or should be retried.

As ocfs is an extent-based filesystem, a bitmap of all extents in the
filesystem is stored just after the vote sectors.  The extent size is
chosen at format time, and the maximum number of possible extents is
bounded at 8 million (since the global bitmap is 1MB max).  When
choosing a cluster size, one should take into consideration the maximum
size to which the ocfs partition is likely to grow, and also the degree
of contiguousness desired.  For instance, a standard ocfs clustersize is
128 KB, which means that even with a highly fragmented file you will at
least get 128k of oracle data in one contiguous chunk.  On a 32GB
partition, you would end up with about 32 thousand total extents with a 
1mb cluster size, 250 thousand with a 128k cluster size, and 8 million
with a 4k cluster size.  If you plan on having many small files (not
recommended) or plan on using autoextended datafiles, you may wish to
format with a smaller cluster size to maximize the number of total
extents.  If you plan on having fewer files of large size and intend to
manually extend these files in large chunks, you should choose a larger 
cluster size to increase contiguousness.

The system files which follow the global bitmap a laid out as follows:

+---+---+---+---+---+---+---+---+
|   |   |   |   |   |   |   |   |
+---+---+---+---+---+---+---+---+
  ^   ^   ^   ^   ^   ^   ^   ^
  |   |   |   |   |   |   |   |
  |   |   |   |   |   |   |   +-- cleanup log
  |   |   |   |   |   |   +-- recover log
  |   |   |   |   |   +-- file extent bitmap
  |   |   |   |   +-- file extent data
  |   |   |   +-- dir bitmap
  |   |   +-- dir data
  |   +-- volume metadata log 
  +-- volume metadata 

For each system file type there are 32 file entries, one for each
possible node in the cluster.  The first two types (volume metadata and
volume metadata log) are not used in the current version of ocfs.

The directory data and directory bitmap system files are used to
sub-allocate space from the global bitmap for use as directory
structures.  These structures are always 128k in size.  Space is
allocated to the directory data file from the global bitmap in chunks, 
then allocated by checking against the directory bitmap for free bits.

The file extent data and file extent bitmap system files serve a similar
purpose to the directory system files, except that they sub-allocate in
512 byte segments.  These structures are used by ocfs for creating
indirect extent trees for files which are fragmented beyond 3 extents.
Space is directly allocated to these files from the global bitmap, and
the file extent bitmap files keep track of free and used 512 byte
chunks.

The two logfiles together constitute a per-node journalling system. We
journal filesystem metadata changes so that in the case of a
catosrophic event (system crash, power outage, etc) we can recover the
filesystem quickly and cleanly.
Both logfiles consist of a stream of records. Each one has distinct
record types, though some record types are shared.

The recover log is filled with records which will be processed in the
case that we have to abort an operation. In this case, the records in
the cleanup log pertaining to that operation would be ignored.

The cleanup log is filled with records which will be processed in the
case that we want to commit an operation. In this case, the records in
the recover log pertaining to that operation would be ignored.

Each node has two log files, and in the case that a node crashes, one
of the other live nodes in the cluster will process it's logs in order
to complete or abort any operations it was doing.

Finally, following the system files, the actual directory hierarchy
begins:

+---+---+---+        +---+---+
|   |   |   | ...... |   |   | 
+---+---+---+        +---+---+
  ^   ^   ^            ^   ^
  |   |   |            |   | 
  |   |   |            |   +-- file entry #255
  |   |   |            +-- file entry #254
  |   |   +-- file entry #2
  |   +-- file entry #1
  +-- directory header

The entire directory structure is 128k in size, made of 256 512-byte
entries.  When a directory fills beyond 255 files, another 128k
structure must be allocated somewhere on disk from the directory
data/bitmap system files.  This pointer will be chained on to the first
directory structure using a field in the directory header.  This header
also contains information about the number of valid file entries, and
also an alpha-sorted index of filenames within this 128k structure.

Each file entry contains all the information needed to make up a valid
inode: name, size, type, permissions, etc.  It also contains extent
information which describes pointers to each of the extents which make 
up the total allocation for the file.  Three of these extents can exist
in the file entry itself, and will be migrated to an indirect tree
structure if the file grows beyond the third extent.

In addition to the fields described, all file entries (including those
for each system file), each directory header, and some additional
special sectors used for locking the global bitmap and other structures
all contain a disk lock structure.  This is the persistent lock
information used by any process requesting a lock on a filesystem
structure.  This locking info includes the node number of the current
lock owner, the lock level, and a bitmap of nodes which have an open
file handle to a given file entry.
  

  * using debugocfs to inspect on-disk structures

To debug any hangs or inconsistencies that you may find while using
ocfs, the debugocfs tool can be used to inspect all of the filesystem
structures above.  When using this tool, you need to have read access to
the device on which the filesystem was created.  If you are debugging an
issue in which you suspect that some degree of caching is preventing a
node from seeing the most current data (ex. file size does not match
on two separate nodes), then you should bind a raw device to the block
device first and run debugocfs on this raw device instead.

       -g: global bitmap
       -l: full listing of all file entries
       -v: vote sector
       -p: publish sector
       -d: ocfs_dir_node structure for a given path
       -f: ocfs_file_entry structure for a given file
       -s: suck file out to a given location
       -a: file allocation system file
       -A: dir allocation system file
       -b: file allocation bitmap system file
       -B: dir allocation bitmap system file
       -r: recover log file system file
       -c: cleanup log system file
       -L: vol metadata log system file
       -M: vol metadata system file
       -n: perform action as node number given


* explanation of features
  * -h : volume header
Displays the first 2 sectors on an ocfs volume: the vol disk header and
volume label structures.  The pointers in the disk header (ex.  bitmap_off) 
should not change after the first mount.
  * -g : global bitmap
Shows statistics about the global bitmap and the state of each bit
within it.
  * -v : vote sector
Prints the last vote written out by the selected node.  This will show
the disk offset (in bytes) to the resource upon which the lock was
requested, the sequence number of the lock (used to ensure uniqueness),
the vote cast and the node for which the vote was cast.
  * -p : publish sector
Shows any vote request for a particular node (the vote, vote_type, 
vote_map, seq_num, and dir_ent fields will be set) as well as an updated 
timestamp for this node.  This timestamp is not actually a date (and 
debugocfs may erroneously show it as such) but merely a tick count which 
must continually increment for other nodes to acknowledge that this node 
is still alive.  When watching the publish and vote sectors for lock 
requests, the dir_ent field will tell you which disk offset is being 
requested.
  * -f : ocfs_file_entry structure
  * -F : ocfs_file_entry and ocfs_extent_group structures
Prints several fields for a given inode within the filesystem.  The path
used for this file should be relative to the mountpoint.  For example,
if an ocfs filesystem is mounted at /ocfs, the file in question is
/ocfs/a/b/c, and the filesystem is on device /dev/sda1, you should run 
"debugocfs -f /a/b/c /dev/sda1".  This will display not only the lock
state info for the file (curr_master, file_lock, oin_node_map, seq_num),
but also the normal unix file information such as size, owner,
permissions, filename, creation/modification date, and file type flags.
In addition, this command will show information regarding the data
extents used to piece the file data together (up to 3 extents with
file_off, num_bytes and disk_off), the alloc_size (the maximum size to
which the file can grow before more extents need to be added), and the
local_ext and granularity flags.  If local_ext is true (and granularity
is -1) then the 3 extents listed point directly to the data.  Using this
information one could dump num_bytes of data, seeking disk_off bytes into
the device for each of these extents in order to piece the file data
together.  If local_ext is false, the granularity will represent the
number of levels in an indirect tree above the actual pointers to extent
data.  If the -F option is used (instead of -f), a printout of the
entire extent tree will be given, which will include the file entry at
the top of the tree, extent data (EXTDAT) structures at the bottom of
the tree pointing directly to data, and if necessary some extent header
(EXTHDR) structures in the middle of the tree pointing to either more
EXTHDRs or EXTDATs, depending upon the level within the tree.  This
command may be used on any type of file, including directories.  When
specifying a directory make sure to use a trailing slash (/).
Directories will only have one extent listed, and the disk_off for this
extent will point to the beginning of the actual 128k directory block.
  * -d : ocfs_dir_node structure
Shows the header information for a directory (first 512 bytes).  This
will show the number of valid entries in this 128k block (num_ent_used),
the pointer to the next block in the directory chain (next_node_ptr, if
more than 255 file entries), and several fields used for effecient
deletion and renaming of files (first_del, num_del).  In addition, the
alphasorted index of filenames will be shown as well as the index_dirty
flag which shows whether the index needs to be recreated.  The
alloc_file_off and alloc_node fields are used when a directory is
deleted to determine to which dir alloc system file to release the
space.  The node_disk_off field points back up to the file entry for
this directory.  The syntax for this command is exactly like the -f/-F
commands, and one should also ensure that the directory name uses a
trailing slash (/).

  * -a : file allocation system file
  * -A : dir allocation system file
  * -b : file allocation bitmap system file
  * -B : dir allocation bitmap system file
  * -r : recover log file system file
  * -c : cleanup log system file
  * -L : vol metadata log system file
  * -M : vol metadata system file
These options show the ocfs_file_entry structure for the requested
system file for any number of nodes.  The fields shown are exactly the
same as those in the -f/-F options.  For the bitmap types, special
bitmap statistics and free/set bit information will also be shown.


* usage scenarios

  * Before you get started
Before you encounter any problems with an ocfs filesystem, it would 
be wise to inspect the disk header and familiarize yourself with the 
values shown (or save the output to a file), since they will not 
change after the first mount.  To print this sector, run:
  debugocfs -h /dev/device
For recovery purposes, you may even want to save off this critical
sector in case it ever becomes corrupted using the following command:
  dd if=/dev/device of=diskhdr.bak bs=512 count=1
and if you need to restore this sector:
  dd if=diskhdr.bak of=/dev/device bs=512 count=1

  * Erratic behavior or unable to mount
If you are unable to mount an ocfs partition or it seems that there is
inconsistent behavior with the filesystem, the first thing to check is
the volume header.  Check the header using this command:
  debugocfs -h /dev/device
All of the _off parameters are byte offsets on the device and should be
within range for that device (should all be less than device_size).  The
cluster_size, permissions, owner and mountpoint should match the values
chosen at format time.  The following fields will always be zero:
serial_num, start_off, root_bitmap_off, root_bitmap_size, root_size,
dir_node_size, file_node_size.  If any of these fields looks as if it
has been overwritten or zeroed, you should consider restoring at least
the first block of the disk as soon as possible.  The output from this
debugocfs command should be included in a bug report.
If the header is intact, other structures to check include the cleanup
and recover log files.  Before running debugocfs, first determine your
node number from the output of:
  cat /proc/ocfs/##/mountpoint  and
  cat /proc/ocfs/##/nodenum
matching up the mountpoint to determine the proper mount number.  Once
you have determined your nodenum for this volume, you can run:
  debugocfs -r nodenum -c nodenum /dev/device
to inspect the recover and cleanup logs for this node.  Under normal
circumstances, the file_size for each of these should be zero.  During
an active transaction this size may increase to as much as about 1MB,
but if it grows much beyond this the time to complete recovery may be
very long and seem like a hang.  You can watch this file size over time
to ensure that it is reducing using a command such as:
  watch -d "debugocfs -r nodenum -c nodenum /dev/device | grep file_size" 
You may also specify a list or range of node numbers (ex. 7,8-10).

  * File size, last modified, etc. for a file is incorrect
Due to the nature of a clustered filesystem it is sometimes a challenge
to keep the caching that the linux VFS provides from interfering.  You
may at some point encounter a file which has different inode properties
on different nodes.  To determine what is actually committed to disk,
versus what is currently visible in the filesystem, you should first run
an 'ls -lai filename', and then run:
  debugocfs -f /path/to/file /dev/device
on each node.  If the debugocfs information is different between the two
nodes also, you may need to first bind the device to a raw device first
in order to see the actual data on disk (this will ensure that an io is
actually completed, bypassing the linux pagecache).  Read the man(8)
manpage for info on how to bind a raw device, and make sure to download
the debugocfs fixed for raw device support (ARU #XXXXXXX).
Once you have collected all of this information you should submit a bug
with all of this data and information on what filesystem actions were
done on this file and on which nodes.  (ex. datafile was created on node
0, extended by node 1, then truncated by node 0)  The disk should always
hold the "correct" information, so unmounting the filesystem and
remounting it is likely to correct such a problem.

  * Deleted file shows in a directory
  * Non-deleted file does not show
  * Single filename shows up more than once
These types of issues suggest that there is a problem with either the
flags on a file entry or the index of the parent directory.  You will
have to inspect several different filesystem structures to get a
complete picture.  First, run:
  debugocfs -d /parent/directory/ /dev/device    and
  debugocfs -f /parent/directory/filename /dev/device
From the '-d' output, if next_node_ptr is INVALID_NODE_POINTER then the
directory is allocated in only one block (<256 file entries).  Otherwise
this field will point to the next directory block in the chain.  To dump
each successive block you can execute:
  debugocfs -X -h hi -l lo -t ocfs_dir_node /dev/device
where hi and lo represent the high and low order 32-bit values which
make up the 64-bit quantity in next_node_ptr (ex. 34629255680 is hi=8,
lo=269517312).  This can be done for each block in the directory chain
to find files deeper than the first block.  
The '-f' output will either show you a normal file entry or it may not
show any output at all.  If it does not print the file entry this means
that the filename was not found in any of the indexes of the
directories.  It may be that the name can still be found somewhere
within one of the blocks, but the index has been updated to no longer
include this entry when searching or listing.  If you need to look at
the raw data in the directory, you can do:
  dd if=/dev/device of=dirdump bs=512 count=256 skip=<dir offset/512>
for any/all of the directory blocks.  Or another method that can be used
is to use the
  debugocfs -X -h hi -l lo -t ocfs_file_entry /dev/device
command to "cast" a given disk block to the ocfs_file_entry type.  As an
example, let's say that the directory block you are looking at is at
offset 34629255680.  This means that there will be a 512-byte header at
that offset, followed by 255 512-byte file entries.  Also assume that
the num_ent_used in this case is 9, and the index looks like 
"2   4   0   7   3   1   5   6   8".  To look at each file entry in
order, begin with the dir offset (34629255680), then add 512 to skip
over the header (34629256192).  This is the location of entry #0, and
each subsequent entry will follow 512 bytes later.  To examine each of
the 9 files in this directory you could run (in bash):
  first=34629256192; hi=0; lo=0; off=0; fourgb=4294967296
  for i in 2 4 0 7 3 1 5 6 8; do 
    off=`echo "$first + ($i * 512)" | bc`
    hi=`echo "$off / $fourgb" | bc`
    lo=`echo "$off % $fourgb" | bc`
    debugocfs -X -h $hi -l $lo -t ocfs_file_entry echo $off $hi $lo
  done
If you need to inspect file entries which seem to have been deleted (and
thus do not show up in the index), you can change the for statement to:
  for ((i=0; i<255; i++)); do ...
This will allow you to look at every block within the directory
regardless of whether it is currently valid.  Information to look out
for include the sync_flags field (should be OCFS_SYNC_FLAG_VALID on a
normal file entry, may be one of the following for a deleted file: 
OCFS_SYNC_FLAG_DELETED, OCFS_SYNC_FLAG_MARK_FOR_DELETION, 
OCFS_SYNC_FLAG_NAME_DELETED) and the modify_time.  This may give you
some indication of what caused the problem and when it occurred.
If you encounter a situation where a filename appears more than once
when doing a readdir (ls) it is very likely that there is a repeated
entry within the directory index (ex. 2 4 0 7 7 1 5 6 8, 7 repeats).
This may also manifest itself as a spuriously deleted file since this
type of corruption would likely wipe out one file entry in order to
double up another.
Finally, you may see a problem when doing an 'ls -l' where it seems that
the readdir() command has correctly returned one or more filenames, but
the subsequent stat() that ls does on each file will fail on at least
one of these.  This will show up as stderr output at the top of the 'ls'
listing similar to:
  ls: filename: No such file or directory
This error indicates that the index within a directory block is in need
of reindexing, and while all the files within the directory are still ok
and intact, the alphasorted index needs to be resorted.  During certain
operations on the directory (create, delete, rename) the index_dirty
flag will get set to indicate that this needs to be recreated, but in
certain very rare crashes it has been found that this fails to happen.


  * Hang occurs while extending/truncating/removing a datafile
Another common case where database administrators have seen issues when
running ocfs has been in deadlock conditions or runaway lock requests
when trying to get an exclusive lock on a datafile.  In some cases this
has been found to even lock out all future lock requests on that node,
including such things as doing an 'ls' on a directory.
To diagnose such an issue, you should first examine the file entry on
which the hang is happening using the methods above:
  debugocfs -f /path/to/file /dev/device
Of particular interest here are the curr_master and file_lock fields.
The curr_master may be any valid node in the system or may be -1 (or
INVALID) meaning that noone has ownership of the lock.  The file_lock
may be one of OCFS_DLM_ENABLE_CACHE_LOCK, OCFS_DLM_EXCLUSIVE_LOCK,
OCFS_DLM_SHARED_LOCK, or OCFS_DLM_NO_LOCK.  For the current version of
ocfs, the ENABLE_CACHE and EXCLUSIVE levels should be treated the same,
meaning that a particular node has an exclusive lock on this resource
and has full write access to the metadata.  The SHARED lock is only used
for readdir() operations currently, and NO_LOCK means that there are no
nodes currently with a lock on this entry.
Additionally, you will want to look at the publish and vote sectors for
all nodes that you suspect are involved in the hang.  For instance, if
you believe that node numbers 0 and 3 are deadlocked on a resource, you
should run:
  debugocfs -v 0,3 -p 0,3 /dev/device
to see which node is requesting votes (in their publish sector) and what
the other node (the voting node) is responding with.  Unless a node is
responding with FLAG_VOTE_NODE (in the vote slot for the requesting
node) or FLAG_VOTE_OIN_UPDATED then the requesting node may be forced to
continue retrying the lock until the conflict is resolved.  To watch
these values change over time, try:
  watch -d --interval=1 "debugocfs -v 0,3 -p 0,3 /dev/device"
in order to see if the lock sequence number is changing, and whether
only one node is requesting votes (and starving out others).


* Disclaimer
Note that while all of these techniques can be used to help Oracle
Support or Engineering determine the root cause of a problem with ocfs,
one should not attempt to modify an ocfs filesystem by hand, and could
result in further corruption of the filesystem.  Only attempt such an
action if specifically asked to by Support.  With that said, it is
worthwhile to note that it is extremely rare that one will encounter any
problems whereby an oracle datafile's *data* will actually be corrupted.
Most hangs, file entry and directory entry corruptions can be repaired,
if necessary, for the purposes of recovering file data.
